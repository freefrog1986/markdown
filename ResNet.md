## 论文相关
本文是经典论文残差网络的翻译、注释版。
论文题目：Deep Residual Learning for Image Recognition
论文意义：2015年微软研究院提出了残差网络，解决了如何打造更深层深度网络的问题。
论文地址：https://arxiv.org/pdf/1512.03385.pdf

## 摘要
更深的神经网络往往更难以训练，我们提出一个**残差学习框架**，使训练比以往更深的网络变得更加轻松。我们明确地将这些层重新规划为**学习与层的输入有关的残差函数**，而不是学习不相关的函数。
该函数的输入的赋值，并参考层输入，而不是学习未引用的函数。我们提供了非常全面的实验数据来证明，残差网络更容易优化，并且可以在深度增加的情况下让精度也增加。在ImageNet的数据集上我们评测了一个深度152层（是VGG的8倍）的残差网络，但依旧拥有比VGG更低的复杂度。这些残差网络在ImageNet测试集达到了3.57%的错误率，这个结果获得了ILSVRC2015的分类任务第一名，我们还用CIFAR-10数据集分析了100层和1000层的网络。   
对于很多视觉识别任务来说，**表征的深度**是至关重要的。我们极深的网络让我们在COCO的目标检测数据集上得到了28%的相对提升。同时，深度残差网络也是提交参加ILSVRC和COCO2015比赛的基础。我们还赢得了ImageNet目标检测、ImageNet目标定位、COCO目标检测和COCO图像分割等任务的第一名。

## 一.简介
深度卷积神经网络使得图像分类问题上的研究向前飞跃了一大步，深层网络自然地将低/中/高级的特征和分类器集成到 **端到端**的多层方式中，而特征的“层次”的可以通过叠加层(深度)的数量来丰富。最近的证据表明，网络深度是至关重要的，而具有挑战性的ImageNet数据集的主要结果都利用了“非常深”的模型，深度为16到30。其他一些计算机视觉的问题也受益于非常深的网络模型。   
受到深度的重要性的驱动，出现了这样一个问题：是不是堆叠更多的层就一定能得到更好的网络？这个问题的一大障碍就是臭名昭著的梯度消失/爆炸问题，它从网络结构的一开始就阻碍收敛。然而这个问题，很大程度上可以通过 **归一化的初始化**和 **中间层的归一化**解决，这些方法确保几十层的网络能够在使用 **随机梯度下降（SGD）**的后向传播过程中收敛。   
当更深的网络能够开始收敛时，一个 **退化问题**就暴露出来了: 随着网络深度的增加，准确度就会饱和(这可能不足为奇)，然后就会迅速下降。出人意料的，退化问题并不是过拟合导致的，并且增加更多的层反而会导致更多的训练误差，就像文章[11,42]中说的那样，通过我们的实验也得到证实。图1展示了一个典型的例子。   
![Figure 1](http://upload-images.jianshu.io/upload_images/145616-fb67b4fbdb0670c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
训练精度的退化表明，不是所有的网络都同样容易优化。让我们考虑一个浅层网络和与之对应的增加了更多层的深层网络。有一个方法来构建该深层网络：所有添加的层都是 **恒等映射**，其他层则是从训练好的浅层网络复制过来。我们推测采用这种构建方法搭建的两种网络，深层网络应该不会比浅层网络产生更高的训练误差。但实验结果表明，我们手头上有的方案都找不到很好的解，找不到更好或者同样好的解（或者是无法在可接受的时间里做完）。   

*译者注：这一段有点难以理解，用通俗的话说是这样的：现在我们想让神经网络每隔几层就拟合这个函数：y=x，再简单不过了吧？可惜神经网络表现很差！*  

在本文中，我们通过引入一个 **深度残差学习框架**解决这个退化问题。我们不期望每几个叠加的层能直接匹配一个映射，而是明确的让这些层去匹配 **残差映射**。更正式的说法是，这里用H(X)来表示最优解的映射，但我们让堆叠的非线性层去拟合另一个映射$F(X):=H(X)-X$ , 此时原最优解映射 $H(X)$就可以改写成 $F(X)+X$，我们假设残差映射跟原映射相比更容易被优化。现在我们考虑极端情况，如果恒等映射是最优解，那么可以简单的将残差置0，把残差置0和用‘非线性层拟合’相比要简单的多。   

F(X)+X的公式可以通过在前馈网络中做一个“快捷连接”来实现（如图2） ，快捷连接跳过一个或多个层。在我们的用例中，快捷连接简单的执行自身映射，它们的输出被添加到叠加层的输出中。自身快捷连接既不会添加额外的参数也不会增加计算复杂度。整个网络依然可以用SGD+反向传播来做端到端的训练，并且可以很容易用大众框架来实现（比如Caffe）不用修改slover配置（slover是caffe中的核心slover.prototxt）


Figure 2 残差网络：一个结构块
　　我们目前用ImageNet的数据集做了很多综合实验，来证实退化问题和评估我们的方法。我们发现：1、我们的超深残差网络是很容易去优化的，不过对应的普通网络（简单的堆叠层）当深度增加时，表现出更高的错误误差。2.我们的深度残差网络可以轻松的享受深度增加带来的精度增加，产生的效果要远远优于以前的那些网络们。类似的现象在CIFAR-10数据集的实验中也一样，这表明着优化是困难的，我们提出的训练方法在这个数据集中，超过100层的网络表现很成功，还可以扩展到1000层。

　　在ImageNet对象分类数据集上，我们用深度残差网络获得了很棒的结果，我们152层的残差网络是ImageNet的参赛网络中最深的，然而却拥有比VGG更低的复杂度。我们最终的效果是测试集上3.57%的错误率，以此摘取了ILSVRC2015对象分类的第一名。这种超级深的表示方法在其他识别任务中也有良好的泛化能力，使我们进一步赢得了多个比赛的第一名（有ImageNet detection, Imagenet localization,COCOdetection COCOsegmentation），这般有利的证据证明残差学习的原则是可泛化的，我们同样期望残差学习的方法能用在其他的视觉和非视觉问题上。
